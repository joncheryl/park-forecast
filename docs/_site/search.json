[
  {
    "objectID": "report.html",
    "href": "report.html",
    "title": "National Park Visitation Forecasting",
    "section": "",
    "text": "I was reading an article about forecasting visitation to National Parks where it was claimed that the Park Service uses a simple rolling 5 year trend for visitation forecasting 1. This surprised me. Certainly, they would use a more sophisticated forecasting method. Soon after, I discovered a dataset of reservations at national parks from recreation.gov which I suspected could be used to forecast visitation. This proved to be true by utilizing a “pickup” method to be described below. The results are promising and could be further strengthened with more data of higher quality which almost certainly exists."
  },
  {
    "objectID": "report.html#introduction",
    "href": "report.html#introduction",
    "title": "National Park Visitation Forecasting",
    "section": "",
    "text": "I was reading an article about forecasting visitation to National Parks where it was claimed that the Park Service uses a simple rolling 5 year trend for visitation forecasting 1. This surprised me. Certainly, they would use a more sophisticated forecasting method. Soon after, I discovered a dataset of reservations at national parks from recreation.gov which I suspected could be used to forecast visitation. This proved to be true by utilizing a “pickup” method to be described below. The results are promising and could be further strengthened with more data of higher quality which almost certainly exists."
  },
  {
    "objectID": "report.html#data-sources",
    "href": "report.html#data-sources",
    "title": "National Park Visitation Forecasting",
    "section": "Data Sources",
    "text": "Data Sources\n\n\nCode\nimport datetime\nimport sqlite3\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom statsmodels.tsa.ar_model import AutoReg, ar_select_order\n\n##################\n# Setup parameters\n##################\nLEADTIME = datetime.timedelta(180)\nPARK_CODE = \"JOTR\"\nPARK_NAME = \"Joshua Tree National Park\"\n\n########################\n# Import visitation data\n########################\nvisits = pd.read_csv(\n    \"../data/nps_visits.csv\").astype({\"date\": \"datetime64[s]\"})\nvisits_jt = visits.loc[visits[\"park_code\"] == PARK_CODE].set_index(\"date\")[\n    \"visits\"]\n\n#########################\n# Import reservation data\n#########################\nconn = sqlite3.connect(\"../data/reservations/reservations.db\")\n\nSQL_QUERY = (\n    \"\"\"\n      SELECT parentlocation, park, facilityid, startdate, enddate, orderdate,\n        numberofpeople\n      FROM reservations\n      WHERE parentlocation = ?\n      AND orderdate NOT LIKE '0%'\n      AND startdate NOT LIKE '0%'\n      AND enddate NOT LIKE '0%';\n    \"\"\"\n)\ndf = pd.read_sql_query(SQL_QUERY, conn, params=(PARK_NAME,))\nconn.close()\n\n############\n# Clean data\n############\n# For numberofpeople, replace nulls with mode\nmode_value = df[\"numberofpeople\"].mode().iloc[0]\ndf[\"numberofpeople\"] = df[\"numberofpeople\"].fillna(mode_value).astype(int)\n\n# Convert strings to datetime\ndate_cols = [\"startdate\", \"enddate\", \"orderdate\"]\ndf[date_cols] = df[date_cols].apply(pd.to_datetime)\n\n# Drop reservations where start date is before order date\ndf = df.loc[~(df[\"startdate\"] &lt; df[\"orderdate\"])]\n# Drop reservations where start date is after end date\ndf = df.loc[~(df[\"startdate\"] &gt; df[\"enddate\"])]\n\n# Cast facilityid as category\ndf = df.astype({\"facilityid\": \"category\"})\n\n\nData for visitations was scraped from the NPS site. It includes monthly recreation visitation counts for each national park unit since 1979 or the parks opening, whichever is later.\nThe reservation data was downloaded from recreation.gov2. There are many fields in the reservation datasets but for the purposes of this report, only the following fields were used: park unit, reservation start date, end date, order date, and number of people per reservation.\nScripts for the data download, cleaning, and database creation can be found on the github repo."
  },
  {
    "objectID": "report.html#model-explanation",
    "href": "report.html#model-explanation",
    "title": "National Park Visitation Forecasting",
    "section": "Model Explanation",
    "text": "Model Explanation\n\n\n\n\n\n\nWarning\n\n\n\nLots of equations in this section.\n\n\nThere isn’t much information about pickup forecasting online. This is the paper that I used the most to derive details. The pickup model hinges on using reservations to inform the forecast of usage: the idea being that the relationship between reservations and usage in the past can be used with current reservations to predict future usage.\nAs an example, suppose on July 3, 2025 a hotel had 25 rooms booked for the night of July 17, 2025, (14 days later) and is looking to forecast the actual number of rooms ultimately booked. The previous year, on July 3, 2024, there were 23 rooms booked for the night of July 17, 2024 and 100 rooms were ultimately booked. Here, the “pickup”3 is \\[ \\frac{100 \\mbox{ rooms}}{23 \\mbox{ rooms}} \\approx 4.35 \\]\nwhich is the ratio of rooms that are “picked up”. One could then estimate the ultimate bookings for July 17, 2025 by multiplying this pickup by the number of reservations for July 17, 2025, 14 days prior: \\[ \\hat{f} = 4.35 \\cdot 25 \\mbox{ rooms} = 109 \\mbox{ rooms} \\]\nMore formally, let \\(f_p(t+h)\\) represent the forecast of visitors at national park \\(p\\) during month \\(t+h\\) where the forecast is made at time \\(t\\) with a lead time of \\(h\\). The pickup model used here is defined by \\[ f_p(t+h) = \\pi_p(t+h) \\cdot \\hat{r}_p(t+h) \\]\nwhere \\(\\pi_p(t+h)\\) is the pickup used for month \\(t+h\\) with lead-time \\(h\\) and \\(\\hat{r}_p(t+h)\\) is the forecasted number of reservations for month \\(t+h\\) with lead-time \\(h\\) at national park \\(p\\).\n\nPickup Definition (\\(\\pi_p\\))\nThere are numerous methods for calculating the pickup \\(\\pi_p(t+h)\\) but for the model utilized below, it is defined as follows. Let \\[ R = [r_p(t+h), r_p(t-1\\mbox{ year }+h), ... ] \\]\nrepresent the sequence of reservations for date \\(t\\) at each year in the data and let \\[V = [v_p(t+h), v_p(t-1\\mbox{ year }+h), ... ]\\]\nbe the sequence of visitation counts for date \\(t+h\\) at each year overlapping \\(R\\). We define the pickup for date \\(t+h\\) as \\[ \\pi_p(t+h) = SimpExpSm_\\alpha\\bigg(\\bigg[\\frac{V_i}{R_i}\\bigg]_i\\bigg) \\] where \\(SimpExpSm_\\alpha\\) is the first forecast of simple exponential smoothing model with smoothing factor \\(\\alpha\\).\nIt’s not necessary to use a fancy forecasting model like exponential smoothing for \\(\\pi_p\\). Initially, I was just using the historical average, i.e., \\(\\pi_p(t+h) = \\mbox{ mean}\\big(\\big[\\frac{V_i}{R_i}\\big]\\big)\\). But assigning more weight to more recent pickups produced better forecasts and exponential smoothing does just that. Almost any other time series model could be used here as \\(\\big[\\frac{V_i}{R_i}\\big]\\) is just a time series itself.\n\n\nReservation Forecast Definition (\\(\\hat{r}_p\\))\nTraditionally (does that word apply to this forecasting method? lol) \\(\\hat{r}_p(t+h)\\) is defined as the number of reservations for month \\(t+h\\) at time \\(t\\): \\[ \\hat{r}_p(t+h) = R_i = r_p(t+h) \\]\nbut this proved to be problematic as the sequence contained a fair bit of noise, especially during slow seasons when reservations were low. However, using the historical average was also not ideal as it provided too much significance to historical values of \\(R_i\\). Using a simple exponential smoothing forecast on the sequence \\([R_i]\\) proved useful, reducing noise but not erasing trends. Thus, we define \\[ \\hat{r}_p(t+h) = SimpExpSm_\\alpha\\big(\\big[R_i\\big]\\big) \\]\nwhere \\(\\alpha\\) is the smoothing factor."
  },
  {
    "objectID": "report.html#edapre-visualizations",
    "href": "report.html#edapre-visualizations",
    "title": "National Park Visitation Forecasting",
    "section": "EDA/Pre-visualizations",
    "text": "EDA/Pre-visualizations\nFor the sake of exposition, in this document I’ll limit the analysis to only Joshua Tree National Park. I’ve run the model on other parks and found similar results.\nTo get a broad idea of how correlated reservations are with visitations, we can aggregate reservation start dates by month and plot that against visitations for that month.\n\n\nCode\n#######################################\n# Plot visits and reservations together\n#######################################\n\n# Reservations by year\norders = (\n    df.groupby(df[\"startdate\"].dt.year,\n               observed=True)[\"numberofpeople\"].sum().reset_index())\norders = orders.loc[(orders[\"startdate\"] &gt; 2006) &\n                    (orders[\"startdate\"] &lt; 2024)]\n\n# Visits by year\nvisits_yearly = visits.loc[\n    (visits[\"park_code\"] == PARK_CODE)\n    & (visits[\"date\"].dt.year &gt; 2006)\n    & (visits[\"date\"].dt.year &lt; 2024)\n]\nvisits_yearly = visits_yearly.groupby(\n    visits_yearly[\"date\"].dt.year, observed=True)[\"visits\"].sum()\n\n# Create a new figure and add traces from both\nfig = go.Figure()\n\nfig.add_trace(\n    go.Scatter(\n        x=visits_yearly.index,\n        y=visits_yearly,\n        name=\"Visits\",\n        yaxis=\"y\",\n    )\n)\n\nfig.add_trace(\n    go.Scatter(\n        x=orders[\"startdate\"],\n        y=orders[\"numberofpeople\"],\n        name=\"Reservations\",\n        yaxis=\"y2\"))\n\n# Optional layout tweaks\nfig.update_layout(\n    title=\"Joshua Tree National Park Visitors and Reservations\",\n    xaxis=dict(showgrid=False),\n    yaxis=dict(title=\"Reservations\", showgrid=False,),\n    yaxis2=dict(\n        title=\"Visits\", overlaying=\"y\", side=\"right\", rangemode=\"tozero\",\n        showgrid=False),\n    barmode=\"stack\", template=\"plotly_white\",\n    legend=dict(x=1.1, y=1),)\n\nfig.show()\n\n\n\n\n                            \n                                            \n\n\nFigure 1: Yearly Visitation and Reservations\n\n\n\n\nAnd let’s look at seasonality. The bars represent total average visitation and the lines are yearly reservation counts for each month given a lead time of 180 days:\n\n\nCode\nmonth_order = [\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n               \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"]\n\n# Reservations by month\ndf['monthyear'] = df['startdate'].dt.to_period('M').dt.to_timestamp()\nstart_res = df.groupby('monthyear')['numberofpeople'].sum().reset_index()\n\n# Visits by month\nvisits_monthly = visits.loc[\n    (visits[\"park_code\"] == PARK_CODE)\n    & (visits[\"date\"].dt.year &gt; 2006)\n    & (visits[\"date\"].dt.year &lt; 2024)\n]\nvisits_monthly = (\n    visits_monthly.groupby(\n        visits_monthly[\"date\"].dt.strftime(\"%b\"),\n        observed=True)[\"visits\"].sum().reindex(month_order))\n\nfig = go.Figure()\n\n# Visitation numbers as a bar graph\nfig.add_trace(\n    go.Bar(\n        x=visits_monthly.index,\n        y=visits_monthly,\n        name=\"Visits\",\n    )\n)\n\nstart_years = start_res[\"monthyear\"].dt.year.unique()\nfor year in start_years:\n    year_data = start_res[start_res[\"monthyear\"].dt.year == year]\n    fig.add_trace(\n        go.Scatter(\n            x=year_data[\"monthyear\"].dt.strftime(\"%b\"),\n            y=year_data[\"numberofpeople\"],\n            yaxis=\"y2\",\n            name=str(year),\n            mode=\"lines\",\n        )\n    )\n\n# Optional layout tweaks\nfig.update_layout(\n    title=\"Joshua Tree National Park Average Visitors and Reservations by Month\",\n    yaxis=dict(\n        title=\"Reservations\",\n        showgrid=False,\n    ),\n    yaxis2=dict(\n        title=\"Visits\", overlaying=\"y\", side=\"right\", rangemode=\"tozero\", showgrid=False\n    ),\n    template=\"plotly_white\",\n    legend=dict(x=1.1, y=1),\n)\n\nfig.show()\n\n\n\n\n                            \n                                            \n\n\nFigure 2: Visits and Reservations by Month"
  },
  {
    "objectID": "report.html#results",
    "href": "report.html#results",
    "title": "National Park Visitation Forecasting",
    "section": "Results",
    "text": "Results\nAnd now we fit the model. The two smoothing parameters were chosen based off of experimenting to see which yielded the best results.\n\n\nCode\n######################\n# Prep for forecasting\n######################\n\nbeginning = df[\"startdate\"].min()\nmin_year = beginning.year\nend = df[\"startdate\"].max()\nmax_year = end.year\n\n# Visits for just whatever park we're interested in\nvisits_jt = visits.loc[visits[\"park_code\"] == PARK_CODE].set_index(\"date\")[\n    \"visits\"]\n\n####################\n# Reservation matrix\n####################\ndf[\"startmonthyear\"] = df[\"startdate\"].dt.to_period(\"M\").dt.to_timestamp()\n\n# This is not the exact lead time to the reservation but the lead time to the\n# beginning of the month of the reservation which is what we need for forecasting\n# monthly visitation before that month occurs.\ndf[\"monthleadtime\"] = df[\"startmonthyear\"] - df[\"orderdate\"]\n\n# This is the sum of reservations for each startdate with minimum monthleadtime.\nres_matrix = (\n    df.loc[df[\"monthleadtime\"] &gt;= LEADTIME]\n    .groupby(\"startdate\")[\"numberofpeople\"]\n    .sum()\n    .reset_index()\n)\n\n# Aggregate to monthyears\nres_matrix[\"startmonthyear\"] = (\n    res_matrix[\"startdate\"].dt.to_period(\"M\").dt.to_timestamp()\n)\nres_matrix = res_matrix.groupby(\"startmonthyear\")[\"numberofpeople\"].sum()\n# Fill out the entire daterange index\nfull_index = pd.date_range(\n    start=f\"{min_year}-01-01\", end=f\"{max_year}-12-01\", freq=\"MS\"\n)\nres_matrix = res_matrix.reindex(full_index, fill_value=0)\n\n\n###############\n# Pickup matrix\n###############\ndef pickup(y: int, month_of_year: int) -&gt; float:\n    \"\"\"\n    Calculate pickup given year, month, and leadtime.\n    \"\"\"\n    reservation_count = res_matrix[datetime.datetime(y, month_of_year, 1)]\n    reservation_count = reservation_count.mean()\n\n    if reservation_count &lt; 100:\n        return np.nan\n\n    return visits_jt[datetime.datetime(y, month_of_year, 1)] / reservation_count\n\n\npickup_matrix = {}\nfor year in range(min_year, max_year):\n    for month in range(1, 13):\n        pickup_matrix[(year, month, LEADTIME)] = pickup(year, month)\n\n############################################################\n# Forecast model for month=t with lead time=l\n############################################################\nPICKUP_SMOOTHING = 0.8  # .94 was great when tsa function didn't have +1\nRES_SMOOTHING = 0.6  # .96\n\n\ndef v_forecast(\n        monthyear: datetime.datetime, leadtime: datetime.timedelta) -&gt; float:\n    \"\"\"Multiplicative, (classical/advanced?), historical average pickup model.\n    leadtime is timedelta in days.\n\n    Will only generate a forecast if the month of interest is at least one year after\n    the beginning of the relevant data.\n    Will also only generate a forecast if the month of interest is at least the\n    leadtime after the beginning.\n    \"\"\"\n    if (monthyear &gt; beginning + datetime.timedelta(366)) & (\n        monthyear - leadtime &gt; beginning\n    ):\n        return pickup_est(monthyear, leadtime) * tsa_res_count(\n            monthyear.year, monthyear.month\n        )\n    return 0\n\n\ndef pickup_est(\n        monthyear: datetime.datetime, leadtime: datetime.timedelta) -&gt; float:\n    \"\"\"Estimator of pickup. Currently using a simple exponential smoothing model\n    to generate estimator. This weights recent pickups more heavily\n    \"\"\"\n    yearly_pickups = pd.Series(\n        {\n            datetime.datetime(y, monthyear.month, 1): pickup_matrix[\n                (y, monthyear.month, leadtime)\n            ]\n            for y in range(min_year, monthyear.year)\n        }\n    )\n\n    # If there are never any valid previous pickup values, then return nan\n    if yearly_pickups.empty or yearly_pickups.isna().all():\n        return np.nan\n\n    # If only one usable value, return it\n    non_na_values = yearly_pickups.dropna()\n    if len(non_na_values) == 1 or non_na_values.nunique() == 1:\n        return float(non_na_values.iloc[0])\n\n    # Fill missing values with the mean (or consider ffill/bfill if trend exists)\n    yearly_pickups = yearly_pickups.fillna(non_na_values.mean())\n\n    yearly_pickups.index = pd.DatetimeIndex(\n        pd.to_datetime(yearly_pickups.index)\n    ).to_period(\"Y\")\n    yearly_pickups = yearly_pickups.sort_index()\n\n    model = SimpleExpSmoothing(yearly_pickups).fit(\n        smoothing_level=PICKUP_SMOOTHING, optimized=False)\n    return model.forecast(1).iloc[0]\n\n\ndef tsa_res_count(y: int, month_of_year: int) -&gt; float:\n    \"\"\"\n    Use a exponential smoothing function to forecast what \"should\" be the next reservation count.\n    \"\"\"\n    res_trend = pd.Series(\n        {\n            datetime.datetime(year, month_of_year, 1): res_matrix[\n                datetime.datetime(year, month_of_year, 1)\n            ]\n            for year in range(min_year, y + 1)\n        }\n    )\n\n    # If there is only one year of history, then return that one pickup\n    if len(res_trend) == 1:\n        return float(res_trend.iloc[0])\n    # If there are never any valid previous pickup values, then return nan\n    if res_trend.isna().all():\n        return np.nan\n    # If no trend or level, then return 0 to stop below from complaining.\n    if (res_trend == 0).all():\n        return 0\n\n    res_trend.index = pd.DatetimeIndex(\n        pd.to_datetime(res_trend.index)).to_period(\"Y\")\n    res_trend = res_trend.sort_index()\n    model = SimpleExpSmoothing(res_trend).fit(\n        smoothing_level=RES_SMOOTHING, optimized=False)\n    return model.forecast(1).iloc[0]\n\n\n#######################\n# Model fit and display\n#######################\nvisits_df = visits_jt.loc[(pd.to_datetime(\n    visits_jt.index).year &gt;= 2008)].reset_index()\nvisits_df[\"pickup_model\"] = visits_df[\"date\"].apply(\n    lambda date: v_forecast(date, LEADTIME))\n\nvisits_df = visits_df.melt(id_vars=\"date\", value_vars=[\n                           \"pickup_model\", \"visits\"])\n\npx.line(\n    visits_df.sort_values(\"date\"),\n    x=\"date\",\n    y=\"value\",\n    color=\"variable\",\n    title=f\"Actual Visits to Joshua Tree NP and {LEADTIME.days} Day Lead-time Forecast\",\n    template=\"plotly_white\",\n)\n\n\n\n\n                            \n                                            \n\n\nFigure 3: Actual Visitation vs Forecast\n\n\n\n\nWe can see from this graph that there’s a gross overestimate for forecasting for a few months, eg. Dec 2021, March 2023, etc. This effect can be attenuated by increasing the smoothing factors in the model at the expense of model accuracy at detecting quick changes in visitation (the fast growth beginning at the end of 2014 and continuing through 2019).\n\n\nCode\n# Unmelt table\nmodel_df = visits_df.pivot(index='date', columns = 'variable', values='value')\n\n\ndef mape(actual: pd.Series, forecast: pd.Series) -&gt; float:\n    \"\"\"\n    Mean Absolute Percentage Error\n    \"\"\"\n\n    actual = actual.replace(0, None)\n\n    return ((actual - forecast) / actual).abs().mean() * 100\n\ndef mse(actual: pd.Series, forecast: pd.Series) -&gt; float:\n    \"\"\"\n    Mean Absolute Percentage Error\n    \"\"\"\n\n    actual = actual.replace(0, None)\n\n    return pow(actual - forecast, 2).mean()\n\ndef rsquared(actual: pd.Series, forecast: pd.Series) -&gt; float:\n    \"\"\"\n    R squared\n    \"\"\"\n    ss_res = pow(actual - forecast, 2).sum()\n    ss_tot = pow(actual - actual.mean(), 2).sum()\n\n    return 1 - ss_res / ss_tot\n\nprint(\"Pickup model:\")\nprint(f\"MAPE = {mape(model_df['visits'], model_df['pickup_model']):.2f}\")\nprint(f\"MSE = {mse(model_df['visits'], model_df['pickup_model']):.2f}\")\nprint(f\"R^2 = {rsquared(model_df['visits'], model_df['pickup_model']):.2f}\")\n\n\nPickup model:\nMAPE = 30.86\nMSE = 7654758303.19\nR^2 = 0.11"
  },
  {
    "objectID": "report.html#comparison-to-other-models",
    "href": "report.html#comparison-to-other-models",
    "title": "National Park Visitation Forecasting",
    "section": "Comparison to Other Models",
    "text": "Comparison to Other Models\n\nAR(5) Model\nThe National Park publication referenced above claims to use “trend line extensions based on actual visitation data from the previous five years” but I don’t know where the numbers in the chart (pages 51-61) are coming from. A heirachical autoregressive model was fit in Clark, et al and serves as reference point for establishing a baseline comparison. Here, to capture seasonality and avoid the complexity of more advanced models (eg SARIMA), I’ve created an AR(5) model for each month, i.e. visitation forecasts for month \\(M\\) and year \\(Y\\) are created by fitting an AR(5) model on the time series of visitations on month \\(M\\) and for years preceding \\(Y\\).\n\n\nCode\n####################\n# Five year AR model\n####################\n\nvisits_jt = visits.loc[visits[\"park_code\"] == PARK_CODE].set_index(\"date\")[\n    \"visits\"]\n\nvisits_jt.index = pd.to_datetime(visits_jt.index, errors=\"coerce\")\nvisits_jt = visits_jt.asfreq(\"MS\")\nvisits_jt = visits_jt.sort_index()\n\npredictions = pd.Series(index=visits_jt.index, dtype=float)\nsarima_preds = pd.Series(index=visits_jt.index, dtype=float)\n\n# Iterate over all dates where we want to make a forecast\nfor date in visits_jt.index:\n    target_month = date.month\n    target_year = date.year\n\n    # Subset of past years' same month\n    mask = (visits_jt.index.month == target_month) & (\n        visits_jt.index.year &lt; target_year\n    )\n    subset = visits_jt.loc[mask].dropna()\n\n    if len(subset) &gt;= 12:\n        lag_order = ar_select_order(subset, maxlag=5, ic=\"aic\")\n        model = AutoReg(subset, lags=lag_order.ar_lags, old_names=False)\n        result = model.fit()\n        predictions.loc[date] = result.forecast(1).iloc[0]\n    else:\n        # Not enough data to fit AR(5); optionally set prediction to NaN\n        predictions.loc[date] = np.nan\n\npredictions.name = \"ar_model\"\n\n\nmodels_wide = visits_df.pivot(index=\"date\", columns=\"variable\", values=\"value\")\nmodels_wide = models_wide.merge(predictions, on=\"date\")\n\nplot_df = models_wide.reset_index().melt(\n    id_vars=\"date\", value_vars=[\"visits\", 'ar_model'])\n\npx.line(\n    plot_df.sort_values(\"date\"),\n    x=\"date\",\n    y=\"value\",\n    color=\"variable\",\n    title=f\"Actual Visits to Joshua Tree NP and {LEADTIME.days} Day Lead-time AR(5) Forecast\",\n    template=\"plotly_white\",\n).show()\n\nprint(\"AR(5) model:\")\nprint(f\"MAPE = {mape(models_wide['visits'], models_wide['ar_model']):.2f}\")\nprint(f\"MSE = {mse(models_wide['visits'], models_wide['ar_model']):.2f}\")\nprint(f\"R^2 = {rsquared(models_wide['visits'], models_wide['ar_model']):.2f}\")\n\n\n                            \n                                            \n\n\nAR(5) model:\nMAPE = 16.86\nMSE = 3472426698.64\nR^2 = 0.53\n\n\n\n\nSARIMA\nTo be continued…\n\n\nDirect exponential smoothing\nTo be continued…"
  },
  {
    "objectID": "report.html#potential-improvements",
    "href": "report.html#potential-improvements",
    "title": "National Park Visitation Forecasting",
    "section": "Potential Improvements",
    "text": "Potential Improvements\n\nTaking park facility into consideration\nEach reservation is associated to facility with each park unit. I initially thought it would be useful to weight reservations at new facilities less heavily but the results did not seem improved. However, it’s possible that this was an oversight as the pickup model was not fully developed at the time.\n\n\nMore advanced forecasting models for pickup and reservation trends.\nCurrently using a simple exponential smoothing model for both. Could try a Holt-Winters model (see here) or others that allow for trends. Could also try Holt-Winters that includes a seasonality component.\n\n\nPickup method vs (S)ARIMA(X) model\nCould try forecasting with an ARIMA model, using reservations as an exogenous variable and taking into account the obvious seasonality. The current autoregressive model actually has a lead time of 1 year which puts it at quite a disadvantage to the pickup model. This lead time could be reduced by expanding an AR model to a more complex SARIMA model.\n\n\nHierarchical Modeling\nSee here"
  },
  {
    "objectID": "report.html#conclusion",
    "href": "report.html#conclusion",
    "title": "National Park Visitation Forecasting",
    "section": "Conclusion",
    "text": "Conclusion\nThe autoregressive model is suprisingly effective given it’s simplicity. Currently it outpreforms the pickup model as presented but it should be noted that this is not at all and apples to apples comparison. For one, the autoregressive model here has an inherent lead time of 1 year as opposed to a variable lead time of the pickup method. Additionally, the pickup model is not “fit” on the data. The smoothing parameters can be tuned but this has been done only by ad hoc experimentation. This could be automated and there are certainly many ways in which the current pickup model could be optimized (eg, more elegant noise reduction, MLE fitting of pickup estimators, trying different time series models instead of exponential smoothing, …)"
  },
  {
    "objectID": "report.html#footnotes",
    "href": "report.html#footnotes",
    "title": "National Park Visitation Forecasting",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe evidence provided for this was this document↩︎\nThere’s an API but, at least as of July 2025, it does not contain as much data as is contained in the published csv files↩︎\nThis is a multiplicative pickup as opposed to an additive pickup. This distinction and the distinction between classical vs advanced pickup models is left as an exercise to the reader of the aforementioned paper.↩︎"
  }
]