---
title: "National Park Visitation Forecasting"
toc: true
toc-location: left
format:
  html:
    code-fold: true
    code-tools: true
jupyter: python3
---

## Introduction
I was reading [an article](https://digitalcommons.usu.edu/cgi/viewcontent.cgi?article=1027&context=envs_stures) about forecasting visitation to National parks wherein it was claimed that the Park Service uses a simple rolling 5 year trend for visitation forecasting ^[The evidence provided for this was [this document](https://irma.nps.gov/DataStore/DownloadFile/548275)]. This surprised me. Certainly, they would use a more sophisticated forecasting method. Soon after, I discovered a dataset of reservations at national parks from recreation.gov which I suspected could be used to forecast visitation. This proved to be true by utilizing a "pickup" method to be described below. The results are promising and could be further strengthened with more data of higher quality which almost certainly exists.

## Data Sources
```{python}
import datetime
import sqlite3
import pandas as pd
import numpy as np
import plotly.express as px
import plotly.graph_objects as go
from statsmodels.tsa.holtwinters import SimpleExpSmoothing

##################
# Model parameters
##################
LEADTIME = datetime.timedelta(31)
PARK_CODE = "JOTR"
PARK_NAME = "Joshua Tree National Park"

########################
# Import visitation data
########################
visits = pd.read_csv("../data/nps_visits.csv").astype({"date": "datetime64[s]"})
visits_jt = visits.loc[visits["park_code"] == PARK_CODE].set_index("date")["visits"]

#########################
# Import reservation data
# This took 10.13 seconds originally.
#########################
conn = sqlite3.connect("../data/reservations/reservations.db")

SQL_QUERY = (
    """
      SELECT parentlocation, park, facilityid, startdate, enddate, orderdate,
        numberofpeople
      FROM reservations
      WHERE parentlocation = ?
      AND orderdate NOT LIKE '0%'
      AND startdate NOT LIKE '0%'
      AND enddate NOT LIKE '0%';
    """
)
df = pd.read_sql_query(SQL_QUERY, conn, params=(PARK_NAME,))
conn.close()

############
# Clean data
############
# For number of people, replace nulls with mode
mode_value = df["numberofpeople"].mode().iloc[0]
df["numberofpeople"] = df["numberofpeople"].fillna(mode_value).astype(int)

# Convert strings to datetime
date_cols = ["startdate", "enddate", "orderdate"]
df[date_cols] = df[date_cols].apply(pd.to_datetime)

# Drop reservations where start date is before order date
df = df.loc[~(df["startdate"] < df["orderdate"])]
# Drop reservations where start date is after end date
df = df.loc[~(df["startdate"] > df["enddate"])]

# Cast facilityid as category
df = df.astype({"facilityid": "category"})
```

Data for visitations was scraped from the NPS site. It includes monthly recreation visitation counts for each national park unit since 1979 or the parks opening, whichever is later.

The reservation data was downloaded from recreation.gov^[There's an API but, at least as of July 2025, it does not contain as much data as is contained in the published csv files]. There are many fields in the reservation datasets but for the purposes of this report, only the following fields were used: park unit, reservation start date, end date, order date, and number of people per reservation.

Scripts for the data download, cleaning, and database creation can be found on the github repo.

## Model Explanation
::: {.callout-warning}
Lots of equations in this section.
:::
There isn't much information about pickup forecasting online. [This](https://www.scitepress.org/papers/2016/56319/56319.pdf) is the paper that I used the most to derive details. The pickup model hinges on using reservations to inform the forecast of usage: the idea being that the relationship between reservations and usage in the past can be used with current reservations to predict future usage.

As an example, suppose on July 3, 2025 a hotel had 25 rooms booked for the night of July 17, 2025, (14 days later) and is looking to forecast the actual number of rooms ultimately booked. The previous year, on July 3, 2024, there were 23 rooms booked for the night of July 17, 2024 and 100 rooms were ultimately booked. Here, the "pickup"^[This is a multiplicative pickup as opposed to an additive pickup. This distinction and the distinction between classical vs advanced pickup models is left as an exercise to the reader of the aforementioned paper.] is
$$ \frac{100 \mbox{ rooms}}{23 \mbox{ rooms}} \approx 4.35 $$

which is the ratio of rooms that are "picked up". One could then estimate the ultimate bookings for July 17, 2025 by multiplying this pickup by the number of reservations for July 17, 2025, 14 days prior:
$$ \hat{f} = 4.35 \cdot 25 \mbox{ rooms} = 109 \mbox{ rooms} $$

More formally, let $f_p(t+H)$ represent the forecast of visitors at national park $p$ during month $t+H$ where the forecast is made at time $t$ with a lead time of $H$. The pickup model used here is defined by 
$$ f_p(t+H) = \pi_p(t+H) * \hat{r}_p(t+H) $$

where $\pi_p(t+H)$ is the pickup used for month $t+H$ with lead-time $H$ and $\hat{r_p}(t+H)$ is the forecasted number of reservations for month $t+H$ with lead-time $H$ at national park $p$.

### Pickup Definition ($\pi_p$)
There are numerous methods for calculating the pickup $\pi_p(t+H)$ but for the model utilized below, it is defined as follows. Let 
$$ R = [r_p(t+H), r_p(t-1\mbox{ year }+H), ... ] $$

represent the sequence of reservations for date $t$ at each year in the data and let
$$V = [v_p(t+H), v_p(t-1\mbox{ year }+H), ... ]$$

be the sequence of visitation counts for date $t+H$ at each year overlapping $R$. We define the pickup for date $t+H$ as
$$ \pi_p(t+H) = SimpExpSm_\alpha\bigg(\bigg[\frac{V_i}{R_i}\bigg]_i\bigg) $$
where $SimpExpSm_\alpha$ is the first forecast of simple exponential smoothing model with smoothing factor $\alpha$.

It's not necessary to use a fancy forecasting model like exponential smoothing for $\pi_p$. Initially, I was just using the historical average, i.e., $\pi_p(t+H) = \mbox{ mean}\big(\big[\frac{V_i}{R_i}\big]\big)$. But assigning more weight to more recent pickups produced better forecasts and exponential smoothing does just that. Almost any other time series model could be used here as $\big[\frac{V_i}{R_i}\big]$ is just a time series itself.

### Reservation Forecast Definition ($\hat{r}_p$)
Traditionally (does that word apply to this forecasting method? lol) $\hat{r}_p(t+H)$ is defined as the number of reservations for month $t+H$ at time $t$:
$$ \hat{r}_p(t+H) = R_i = r_p(t+H) $$

but this proved to be problematic as the sequence contained a fair bit of noise, especially during slow seasons when reservations were low. However, using the historical average was also not ideal as it provided too much significance to historical values of $R_i$. Using a simple exponential smoothing forecast on the sequence $[R_i]$ proved useful, reducing noise but not erasing trends. Thus, we define
$$ \hat{r}_p(t+H) = SimpExpSm_\alpha\big(\big[R_i\big]\big) $$

where $\alpha$ is the smoothing factor.

## EDA/Pre-visualizations
For the sake of exposition, in this document I'll limit the analysis to only Joshua Tree National Park. I've run the model on other parks and found similar results.

To get a broad idea of how correlated reservations are with visitations, we can aggregate reservation start dates by month and plot that against visitations for that month.
```{python}
#| label: fig-yearly
#| fig-cap: "Yearly Visitation and Reservations"

#######################################
# Plot visits and reservations together
#######################################

# Reservations by year
orders = (
    df.groupby(df["startdate"].dt.year,
               observed=True)["numberofpeople"].sum().reset_index())
orders = orders.loc[(orders["startdate"] > 2006) &
                    (orders["startdate"] < 2024)]

# visits by year
df_jt = visits.loc[
    (visits["park_code"] == PARK_CODE)
    & (visits["date"].dt.year > 2006)
    & (visits["date"].dt.year < 2024)
]
df_yearly = df_jt.groupby(df_jt["date"].dt.year, observed=True)["visits"].sum()

# Create a new figure and add traces from both
fig = go.Figure()

fig.add_trace(
    go.Bar(
        x=orders["startdate"],
        y=orders["numberofpeople"],
        name="Reservations",
        yaxis="y"))

fig.add_trace(
    go.Scatter(
        x=df_yearly.index,
        y=df_yearly,
        name="Visits",
        yaxis="y2",
        mode="lines+markers",
    )
)

# Optional layout tweaks
fig.update_layout(
    title="Joshua Tree National Park Visitors and Reservations",
    yaxis=dict(title="Reservations", showgrid=False,),
    yaxis2=dict(
        title="Visits", overlaying="y", side="right", rangemode="tozero",
        showgrid=False),
    barmode="stack", template="plotly_white",)

fig.show()
```

And let's look at seasonality:
```{python}
# | label: fig-weekly
# | fig-cap: "Visits and Reservations by Month"

month_order = ["Jan", "Feb", "Mar", "Apr", "May", "Jun",
                    "Jul", "Aug", "Sep", "Oct", "Nov", "Dec"]

# Viz showing visits/reservations by month
orders_monthly = (
    df.groupby(df["startdate"].dt.strftime("%b"))["numberofpeople"]
    .sum()
    .reindex(month_order)
)


# visits by year
df_jt = visits.loc[
    (visits["park_code"] == PARK_CODE)
    & (visits["date"].dt.year > 2006)
    & (visits["date"].dt.year < 2024)
]
visits_monthly = (
    df_jt.groupby(df_jt["date"].dt.strftime("%b"), observed=True)["visits"]
    .sum()
    .reindex(month_order)
)


fig = go.Figure()

fig.add_trace(
    go.Bar(
        x=orders_monthly.index,
        y=orders_monthly,
        name="Reservations",
        yaxis="y",
    )
)

fig.add_trace(
    go.Scatter(
        x=visits_monthly.index,
        y=visits_monthly,
        name="Visits",
        yaxis="y2",
        mode="lines+markers",
    )
)

# Optional layout tweaks
fig.update_layout(
    title="Joshua Tree National Park Average Visitors and Reservations by Month",
    yaxis=dict(
        title="Reservations",
        showgrid=False,
    ),
    yaxis2=dict(
        title="Visits", overlaying="y", side="right", rangemode="tozero", showgrid=False
    ),
    barmode="stack",
    template="plotly_white",
)

fig.show()
```

## Results

And now we fit the model. The two smoothing parameters were chosen based off of experimenting to see which yielded the best results.
```{python}
# | label: fig-model
# | fig-cap: "Visitation vs 31 day lead time forecast"

######################
# Prep for forecasting
######################

beginning = df["startdate"].min()
min_year = beginning.year
end = df["startdate"].max()
max_year = end.year

# Visits for just whatever park we're interested in
visits_jt = visits.loc[visits["park_code"] == PARK_CODE].set_index("date")["visits"]

####################
# Reservation matrix
####################
df["startmonthyear"] = df["startdate"].dt.to_period("M").dt.to_timestamp()

# This is not the exact lead time to the reservation but the lead time to the
# beginning of the month of the reservation which is what we need for forecasting
# monthly visitation before that month occurs.
df["monthleadtime"] = df["startmonthyear"] - df["orderdate"]

# This is the sum of reservations for each startdate with minimum monthleadtime.
res_matrix = (
    df.loc[df["monthleadtime"] >= LEADTIME]
    .groupby("startdate")["numberofpeople"]
    .sum()
    .reset_index()
)

# Aggregate to monthyears
res_matrix["startmonthyear"] = (
    res_matrix["startdate"].dt.to_period("M").dt.to_timestamp()
)
res_matrix = res_matrix.groupby("startmonthyear")["numberofpeople"].sum()
# Fill out the entire daterange index
full_index = pd.date_range(
    start=f"{min_year}-01-01", end=f"{max_year}-12-01", freq="MS"
)
res_matrix = res_matrix.reindex(full_index, fill_value=0)


###############
# Pickup matrix
###############
def pickup(y: int, month_of_year: int) -> float:
    """
    Calculate pickup given year, month, and leadtime.
    """
    reservation_count = res_matrix[datetime.datetime(y, month_of_year, 1)]
    reservation_count = reservation_count.mean()

    if reservation_count < 100:
        return np.nan

    return visits_jt[datetime.datetime(y, month_of_year, 1)] / reservation_count


pickup_matrix = {}
for year in range(min_year, max_year):
    for month in range(1, 13):
        pickup_matrix[(year, month, LEADTIME)] = pickup(year, month)

############################################################
# Forecast model for month=t with lead time=l
############################################################


def v_forecast(monthyear: datetime.datetime, leadtime: datetime.timedelta) -> float:
    """Multiplicative, (classical/advanced?), historical average pickup model.
    leadtime is timedelta in days.

    Will only generate a forecast if the month of interest is at least one year after
    the beginning of the relevant data.
    Will also only generate a forecast if the month of interest is at least the
    leadtime after the beginning.
    """
    if (monthyear > beginning + datetime.timedelta(366)) & (
        monthyear - leadtime > beginning
    ):
        return pickup_est(monthyear, leadtime) * tsa_res_count(
            monthyear.year, monthyear.month
        )
    return 0


def pickup_est(monthyear: datetime.datetime, leadtime: datetime.timedelta) -> float:
    """Estimator of pickup. Currently using a simple exponential smoothing model
    to generate estimator. This weights recent pickups more heavily
    """
    yearly_pickups = pd.Series(
        {
            datetime.datetime(y, monthyear.month, 1): pickup_matrix[
                (y, monthyear.month, leadtime)
            ]
            for y in range(min_year, monthyear.year)
        }
    )

    # If there are never any valid previous pickup values, then return nan
    if yearly_pickups.empty or yearly_pickups.isna().all():
        return np.nan

    # If only one usable value, return it
    non_na_values = yearly_pickups.dropna()
    if len(non_na_values) == 1 or non_na_values.nunique() == 1:
        return float(non_na_values.iloc[0])

    # Fill missing values with the mean (or consider ffill/bfill if trend exists)
    yearly_pickups = yearly_pickups.fillna(non_na_values.mean())

    yearly_pickups.index = pd.DatetimeIndex(
        pd.to_datetime(yearly_pickups.index)
    ).to_period("Y")
    yearly_pickups = yearly_pickups.sort_index()

    model = SimpleExpSmoothing(yearly_pickups).fit(smoothing_level=0.7, optimized=False)
    return model.forecast(1).iloc[0]


def tsa_res_count(y: int, month_of_year: int) -> float:
    """
    Use a exponential smoothing function to forecast what "should" be the next reservation count.
    """
    res_trend = pd.Series(
        {
            datetime.datetime(year, month_of_year, 1): res_matrix[
                datetime.datetime(year, month_of_year, 1)
            ]
            for year in range(min_year, y)
        }
    )

    # If there is only one year of history, then return that one pickup
    if len(res_trend) == 1:
        return float(res_trend.iloc[0])
    # If there are never any valid previous pickup values, then return nan
    if res_trend.isna().all():
        return np.nan
    # If no trend or level, then return 0 to stop below from complaining.
    if (res_trend == 0).all():
        return 0

    res_trend.index = pd.DatetimeIndex(pd.to_datetime(res_trend.index)).to_period("Y")
    res_trend = res_trend.sort_index()
    model = SimpleExpSmoothing(res_trend).fit(smoothing_level=0.8, optimized=False)
    return model.forecast(1).iloc[0]


#######################
# Model fit and display
#######################
visits_df = visits_jt.loc[(pd.to_datetime(visits_jt.index).year >= 2008)].reset_index()
visits_df["vhat"] = visits_df["date"].apply(lambda date: v_forecast(date, LEADTIME))

visits_df = visits_df.melt(id_vars="date", value_vars=["visits", "vhat"])

px.line(
    visits_df.sort_values("date"),
    x="date",
    y="value",
    color="variable",
    title="Actual Visits to Joshua Tree NP and 31 Day Lead-time Forecast",
    template="plotly_white",
)

```

MAPE, MSE, ...
```{python}
# Unmelt table
model_df = visits_df.pivot(index='date', columns = 'variable', values='value')


def mape(actual: pd.Series, forecast: pd.Series) -> float:
    """
    Mean Absolute Percentage Error
    """

    actual = actual.replace(0, None)

    return ((actual - forecast) / actual).abs().mean() * 100

def mse(actual: pd.Series, forecast: pd.Series) -> float:
    """
    Mean Absolute Percentage Error
    """

    actual = actual.replace(0, None)

    return pow(actual - forecast, 2).mean()

print(f"MAPE= {mape(model_df['visits'], model_df['vhat']):.2f}")
print(f"MSE= {mse(model_df['visits'], model_df['vhat']):.2f}")
```

## Comparison to Other Models
- Five year autoregressive
- ARIMA?
- Direct exponential smoothing?

## Potential Improvements

### More advanced forecasting models for pickup and reservation trends.
Currently using a simple exponential smoothing model for both. Could try a Holt-Winters model see [here](https://otexts.com/fpp2/holt.html) or others that allow for trends. Could also try Holt-Winters that includes a seasonality component.

### Pickup method vs ARIMAX model
Could try forecasting with an ARIMAX model, using reservations as an exogenous variable. Compare this with pickup results.

### Hierarchical Modeling
See [here](https://nixtlaverse.nixtla.io/hierarchicalforecast/index.html)

## Conclusion